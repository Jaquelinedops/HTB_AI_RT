# -*- coding: utf-8 -*-
"""spam_classifier_poisoning_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_reh2p1du58e9amc-qDqRrDQgqcfOo5
"""

import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import random

# -------------------------------------------------------------------
# Data Helper

def preprocess_message(message):
    stop_words = set(stopwords.words("english")) - {"free", "win", "cash", "urgent"}
    stemmer = PorterStemmer()

    message = message.lower()
    message = re.sub(r"[^a-z\s$!]", "", message)
    tokens = word_tokenize(message)
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)


def preprocess_dataframe(df):
    df['message'] = df['message'].apply(preprocess_message)
    df = df.drop_duplicates()

    return df

# -------------------------------------------------------------------
# Model Helper

# classify messages by a trained model
def classify_messages(model, msg_df, return_probabilities=False):
    if isinstance(msg_df, str):
        msg_preprocessed = [preprocess_message(msg_df)]
    else:
        msg_preprocessed = [preprocess_message(msg) for msg in msg_df]

    msg_vectorized = model.named_steps["vectorizer"].transform(msg_preprocessed)

    if return_probabilities:
        return model.named_steps["classifier"].predict_proba(msg_vectorized)

    return model.named_steps["classifier"].predict(msg_vectorized)


# train a model on the given data set
def train(dataset):
    # read training data set
    df = pd.read_csv(dataset)

    # data preprocessing
    df = preprocess_dataframe(df)

    # data preparation
    vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))
    X = vectorizer.fit_transform(df["message"])
    y = df["label"].apply(lambda x: 1 if x == "spam" else 0)

    # training
    pipeline = Pipeline([("vectorizer", vectorizer), ("classifier", MultinomialNB())])
    param_grid = {"classifier__alpha": [0.1, 0.5, 1.0]}
    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring="f1")
    grid_search.fit(df["message"], y)
    best_model = grid_search.best_estimator_

    return best_model


# evaluate a given model on our test dataset
def evaluate(model, dataset):
    # read test data set
    df = pd.read_csv(dataset)

    # prepare labels
    df['label'] = df['label'].apply(lambda x: 1 if x == "spam" else 0)

    # get predictions
    predictions = classify_messages(model, df['message'])

    # compute accuracy
    correct = np.count_nonzero(predictions == df['label'])
    return (correct / len(df))

# -------------------------------------------------------------------
# Main

model = train("./manipulated_training_data.csv")
acc = evaluate(model, "./test.csv")
print(f"Model accuracy: {round(acc*100, 2)}%")

import pandas as pd

# Load the training data
df = pd.read_csv('/content/training_data.csv')

# Append " Best Regards, HackTheBox" to spam messages
df['message'] = df.apply(lambda row: row['message'] + ' Best Regards, HackTheBox' if row['label'] == 'ham' else row['message'], axis=1)

# Save the manipulated data to a new CSV file
df.to_csv('/content/manipulated_training_data.csv', index=False)

print("Manipulated data saved to /content/manipulated_training_data.csv")

training_data = pd.read_csv('/content/training_data.csv')

import pandas as pd
import numpy as np

# Load the training data from the file
df = pd.read_csv('training_data.csv')

# Separate spam and ham messages
spam_messages = df[df['label'] == 'spam'].copy()
ham_messages = df[df['label'] == 'ham'].copy()

# Select a percentage of spam messages to manipulate (30% of all spam)
percentage_to_manipulate = 0.3
num_spam_to_manipulate = int(len(spam_messages) * percentage_to_manipulate)

# Randomly select the messages to be manipulated
manipulated_spam = spam_messages.sample(n=num_spam_to_manipulate, random_state=42)

# Append the backdoor phrase and change the label to 'ham'
manipulated_spam['message'] = manipulated_spam['message'].astype(str) + ' Best Regards, HackTheBox'
manipulated_spam['label'] = 'ham'

# Drop the manipulated messages from the original spam set
original_spam = spam_messages.drop(manipulated_spam.index)

# Recombine the DataFrames: non-manipulated spam, original ham, and the new backdoor messages
manipulated_df = pd.concat([original_spam, ham_messages, manipulated_spam])

# Shuffle the final DataFrame to mix the messages and avoid any bias
manipulated_df = manipulated_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Save the manipulated data to a new CSV file
manipulated_df.to_csv('manipulated_training_data.csv', index=False)

print("Manipulated data saved to 'manipulated_training_data.csv'")

training_data.label.value_counts()